{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/briankhor/credit-card-fraud-prediction?scriptVersionId=110450986\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Credit Card Fraud Prediction","metadata":{}},{"cell_type":"markdown","source":"The goal of this work is to apply machine learning technique to train a predictive model on a large dataset of credit card transactions to detect potentially fraudulent credit card transactions. We will employ basic machine learning techniques, models and practices below.\n\nThis documentation is a personal learning documentation in applying ML technique and is adapted from various sources below and I claim close-to-zero originality in this work.\n\n- [Predicting Fraud with TensorFlow](http://www.kaggle.com/code/currie32/predicting-fraud-with-tensorflow)\n- [Credit Card Fraud Detection using Machine Learning & Python](http://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc)","metadata":{}},{"cell_type":"markdown","source":"## Making sense of our data \n\nFirst, we need to import the necessary python packages for data visualization, and we will load our data and clean our data by checking for null values and duplicates. Finally, we will try to visualize the basic features of our data.\n\nWe start by loading the necessary packages for our work.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-15T15:12:33.531675Z","iopub.execute_input":"2022-09-15T15:12:33.532491Z","iopub.status.idle":"2022-09-15T15:12:40.475874Z","shell.execute_reply.started":"2022-09-15T15:12:33.532398Z","shell.execute_reply":"2022-09-15T15:12:40.474807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we import the given data set.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:12:44.080363Z","iopub.execute_input":"2022-09-15T15:12:44.081079Z","iopub.status.idle":"2022-09-15T15:12:48.63647Z","shell.execute_reply.started":"2022-09-15T15:12:44.081031Z","shell.execute_reply":"2022-09-15T15:12:48.635625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will explore the basic features of the data below. First, we look at the headings on the dataset.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:07.028517Z","iopub.execute_input":"2022-09-15T15:13:07.028929Z","iopub.status.idle":"2022-09-15T15:13:07.069147Z","shell.execute_reply.started":"2022-09-15T15:13:07.028895Z","shell.execute_reply":"2022-09-15T15:13:07.067854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We look at the basic descriptions on each column on the data (mean, standard deviations, etc). Note that V1 - V28 are confidential information and are already transformed for its original form.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:10.032225Z","iopub.execute_input":"2022-09-15T15:13:10.032646Z","iopub.status.idle":"2022-09-15T15:13:10.536895Z","shell.execute_reply.started":"2022-09-15T15:13:10.032602Z","shell.execute_reply":"2022-09-15T15:13:10.535638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We next look at the transaction distribution. ","metadata":{}},{"cell_type":"code","source":"Total_transaction = len(data)\nnormal = len(data[data.Class == 0])\nfraudulent = len(data[data.Class == 1])\nfraudulent_percentage = round(fraudulent*100/Total_transaction, 2)\nprint('Total number of transactions: ' + str(Total_transaction))\nprint('Number of normal transactions: ' + str(normal))\nprint('Number of fraudulent transactions: ' + str(fraudulent))\nprint('Percentage of fraudulent transactions: ' + str(fraudulent_percentage))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:15.662321Z","iopub.execute_input":"2022-09-15T15:13:15.663315Z","iopub.status.idle":"2022-09-15T15:13:15.724098Z","shell.execute_reply.started":"2022-09-15T15:13:15.663273Z","shell.execute_reply":"2022-09-15T15:13:15.722974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that we have a highly imbalanced dataset with only 0.17% fraudulent transactions, which is expected since we don't expect our credit card to be involved in a scam/fraud every single day (otherwise many would worry about the financial security provided by financial institutions!).\n\nNext, we check for any null value.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:18.890084Z","iopub.execute_input":"2022-09-15T15:13:18.890804Z","iopub.status.idle":"2022-09-15T15:13:18.927013Z","shell.execute_reply.started":"2022-09-15T15:13:18.890766Z","shell.execute_reply":"2022-09-15T15:13:18.926155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! There is no null value present in the data.\n\nWe will first look at the normal and fraudulent distributions for the 'Time' feature.","metadata":{}},{"cell_type":"code","source":"ax = plt.subplot()\nsns.histplot(data['Time'][data.Class==0], bins=50, color=\"blue\",stat=\"density\")\nsns.histplot(data['Time'][data.Class==1], bins=50, color=\"orange\",stat=\"density\")\nplt.legend([\"Normal\", \"Fraud\"])\nax.set_title('Time')\n             \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:22.824706Z","iopub.execute_input":"2022-09-15T15:13:22.825085Z","iopub.status.idle":"2022-09-15T15:13:23.5093Z","shell.execute_reply.started":"2022-09-15T15:13:22.825051Z","shell.execute_reply":"2022-09-15T15:13:23.508023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Markdown and LaTeX:\nIt seems like time is more or less uniform across both distrubutions, and the peaks are artificial as the density of of order $10^{-5}$ for these putative 'peak', so any variation is probably meaningless and we will drop the time feature.\n\nWe will also check for duplicates (it is possible that duplicate transactions were made at slightly different times). We make a simplifying assumption that if all other identification features (meaning V1 - V28 and amount) are the same, we will count them as duplicates and remove them).","metadata":{}},{"cell_type":"code","source":"data.drop(['Time'], axis=1, inplace=True)\ndata.drop_duplicates(inplace=True)\ndata.shape ","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:26.744299Z","iopub.execute_input":"2022-09-15T15:13:26.744712Z","iopub.status.idle":"2022-09-15T15:13:27.881941Z","shell.execute_reply.started":"2022-09-15T15:13:26.744677Z","shell.execute_reply":"2022-09-15T15:13:27.880884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we look at the distributions of the data for both normal and fraudulent transactions for the rest of the features. ","metadata":{}},{"cell_type":"code","source":"v_features = data.iloc[:, 0:29].columns\n\nplt.figure(figsize=(12, 29*4))\ngs = gridspec.GridSpec(29, 1)\n\nfor i, col in enumerate(data[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.histplot(data[col][data.Class == 0], bins = 50, color=\"blue\", stat=\"density\", kde=True)\n    sns.histplot(data[col][data.Class == 1], bins = 50, color=\"orange\", stat=\"density\", kde=True)\n    plt.legend([\"Normal\",\"Fraud\"])\n    ax.set_xlabel('')\n    ax.set_title('Histogram of feature: ' + str(col))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:13:31.12976Z","iopub.execute_input":"2022-09-15T15:13:31.130198Z","iopub.status.idle":"2022-09-15T15:14:19.684561Z","shell.execute_reply.started":"2022-09-15T15:13:31.130161Z","shell.execute_reply":"2022-09-15T15:14:19.683657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we notice a few features of our data.\n\n1. The amount varies very largely (the difference between max and min), but the data is mostly skewed towards the lower end (both normal and fraudulent). So we need a way to scale this.\n\n2. We will drop other anonymous features where normal and fraud distributions are similar (V13, V15, V20, V22 - V26, V28).\n\nWe will start by scaling the amount column and dropping less distinguishable features.","metadata":{}},{"cell_type":"code","source":"data.drop(['V13', 'V15', 'V20', 'V22', 'V23', 'V24', 'V25', 'V26', 'V28'], axis = 1, inplace=True)\n           \nsc = StandardScaler()\namount = data['Amount'].values\ndata['Amount'] = sc.fit_transform(amount.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:14:56.274962Z","iopub.execute_input":"2022-09-15T15:14:56.27536Z","iopub.status.idle":"2022-09-15T15:14:56.307035Z","shell.execute_reply.started":"2022-09-15T15:14:56.275326Z","shell.execute_reply":"2022-09-15T15:14:56.30607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are done cleaning and organizing (i.e., scaling) data. We will move on to training or dataset.","metadata":{}},{"cell_type":"markdown","source":"## Splitting the dataset for train and test set\n\nIt is a standard practice in ML to split the large dataset available to us into train dataset and test dataset. Train dataset is the dataset that is used to train our neural network architecture/model parameter, while the test dataset is not used in training our NN architecture but we apply the trained model to determine how well our model fare in terms of successfully predicting fraudulent transactions.\n\nThe standard practice for modest (i.e., large but not too large, in the ballpark of 100k - several million data points) sized dataset is to split up the train-test set into 80-20 percent. There is no hard rule in splitting the dataset and one is free to slightly alter the fractions.","metadata":{}},{"cell_type":"code","source":"X = data.drop(['Class'], axis=1).values\nY = data['Class'].values\n\ninput_node = X.shape[1]\n\nX = X.astype('float32')\nY = Y.astype('float32').reshape((-1,1))\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:01.170184Z","iopub.execute_input":"2022-09-15T15:15:01.170618Z","iopub.status.idle":"2022-09-15T15:15:01.262177Z","shell.execute_reply.started":"2022-09-15T15:15:01.170546Z","shell.execute_reply":"2022-09-15T15:15:01.261173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running the model using TensorFlow Neural Network\n\nWe will first train a neural network architecture using TensorFlow package. An advantage of the TensorFlow package is that once we build the forward propagation structure, backpropagation portion of the network can be worked out easily with very few lines of codes. \n\nSince this is a binary classificaton problem, we will be using neural network with stacks of logistic regressions, and each layer consists of linear function followed by RELU activation function (except the last layer, where we will use sigmoid activation function). We will choose to use Adam Optimizers for regularization.\n\nSet number of hidden nodes in each layer with a constant ratio. We will use 4 layers with LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.","metadata":{}},{"cell_type":"code","source":"multiplier = 0.5\nhidden_nodes1 = 25\nhidden_nodes2 = round(hidden_nodes1*multiplier)\nhidden_nodes3 = round(hidden_nodes2*multiplier)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:05.899486Z","iopub.execute_input":"2022-09-15T15:15:05.900259Z","iopub.status.idle":"2022-09-15T15:15:05.905351Z","shell.execute_reply.started":"2022-09-15T15:15:05.900214Z","shell.execute_reply":"2022-09-15T15:15:05.90441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we build the weights and biases for each layer, where we write operations between tensors and initialize variables.","metadata":{}},{"cell_type":"code","source":"def initialize_parameter():\n    zero_initializer = tf.zeros_initializer()\n    initializer = tf.keras.initializers.TruncatedNormal(stddev=0.15)\n\n    # layer 1\n    W1 = tf.Variable(initializer([hidden_nodes1, input_node], dtype=tf.float32))\n    b1 = tf.Variable(zero_initializer([hidden_nodes1, 1], dtype=tf.float32))\n\n    # layer 2\n    W2 = tf.Variable(initializer([hidden_nodes2, hidden_nodes1], dtype=tf.float32))\n    b2 = tf.Variable(zero_initializer([hidden_nodes2, 1], dtype=tf.float32))\n                 \n    # layer 3\n    W3 = tf.Variable(initializer([hidden_nodes3, hidden_nodes2], dtype=tf.float32))\n    b3 = tf.Variable(zero_initializer([hidden_nodes3, 1], dtype=tf.float32))\n                 \n    # layer 4\n    W4 = tf.Variable(initializer([1, hidden_nodes3], dtype = tf.float32))\n    b4 = tf.Variable(zero_initializer([1], dtype=tf.float32))\n    \n    return W1, b1, W2, b2, W3, b3, W4, b4\n\ndef forward_propagation(X, W1, b1, W2, b2, W3, b3, W4, b4):\n    tf.cast(X, tf.float32)\n    z1 = tf.math.add(tf.linalg.matmul(W1,X),b1)\n    y1 = tf.keras.activations.relu(z1)\n    z2 = tf.math.add(tf.linalg.matmul(W2,y1),b2)\n    y2 = tf.keras.activations.relu(z2)\n    z3 = tf.math.add(tf.linalg.matmul(W3,y2),b3)\n    y3 = tf.keras.activations.relu(z3)\n    z4 = tf.math.add(tf.linalg.matmul(W4,y3),b4)\n    y4 = tf.keras.activations.sigmoid(z4)\n    \n    return y4","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:11.664023Z","iopub.execute_input":"2022-09-15T15:15:11.664418Z","iopub.status.idle":"2022-09-15T15:15:11.677142Z","shell.execute_reply.started":"2022-09-15T15:15:11.664386Z","shell.execute_reply":"2022-09-15T15:15:11.676065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the hyperparameters such as number of epochs, learning rate and batch size.","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.0005\nbatch_size = 2048\nnum_epoch = 10\n\nn_train = Y_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:15.520226Z","iopub.execute_input":"2022-09-15T15:15:15.52067Z","iopub.status.idle":"2022-09-15T15:15:15.525552Z","shell.execute_reply.started":"2022-09-15T15:15:15.520632Z","shell.execute_reply":"2022-09-15T15:15:15.524743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we write cost function.","metadata":{}},{"cell_type":"code","source":"def compute_cost(y4, Y):\n    \n    logits = tf.transpose(y4)\n    labels = tf.transpose(Y)\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n    \n    return cost","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:18.488948Z","iopub.execute_input":"2022-09-15T15:15:18.489577Z","iopub.status.idle":"2022-09-15T15:15:18.49515Z","shell.execute_reply.started":"2022-09-15T15:15:18.489542Z","shell.execute_reply":"2022-09-15T15:15:18.493867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we sort or data sets into batches, initialize variables, write backpropagation and choose optimizer, and run our session.","metadata":{}},{"cell_type":"code","source":"x_train = tf.data.Dataset.from_tensor_slices(X_train)\ny_train = tf.data.Dataset.from_tensor_slices(Y_train)\nx_test = tf.data.Dataset.from_tensor_slices(X_test)\ny_test = tf.data.Dataset.from_tensor_slices(Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:20.960344Z","iopub.execute_input":"2022-09-15T15:15:20.960783Z","iopub.status.idle":"2022-09-15T15:15:21.034122Z","shell.execute_reply.started":"2022-09-15T15:15:20.960744Z","shell.execute_reply":"2022-09-15T15:15:21.033144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.zip((x_train, y_train))\ntest_dataset = tf.data.Dataset.zip((x_test, y_test))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n\ntest_accuracy = tf.keras.metrics.CategoricalAccuracy()\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\n\nminibatches = dataset.batch(batch_size).prefetch(8)\ntest_minibatches = test_dataset.batch(batch_size).prefetch(8)\n\ncosts = []\ntrain_acc = []\ntest_acc = []\n\nW1, b1, W2, b2, W3, b3, W4, b4 = initialize_parameter()\n    \nfor epoch in range(num_epoch):\n        \n    epoch_cost = 0.\n    train_accuracy.reset_states()\n        \n    for (minibatch_X, minibatch_Y) in minibatches:\n        with tf.GradientTape() as tape:\n            y4 = forward_propagation(tf.transpose(minibatch_X), W1, b1, W2, b2, W3, b3, W4, b4)\n            tf.shape(y4)\n            tf.shape(minibatch_Y)\n            minibatch_cost = compute_cost(y4, tf.transpose(minibatch_Y))\n                \n        train_accuracy.update_state(minibatch_Y, y4)\n        trainable_variables = [W1, b1, W2, b2, W3, b3, W4, b4]\n        grads = tape.gradient(minibatch_cost, trainable_variables)\n        optimizer.apply_gradients(zip(grads, trainable_variables))\n        epoch_cost += minibatch_cost\n            \n    epoch_cost /= n_train\n    costs.append(epoch_cost)\n    train_acc.append(train_accuracy.result())\n    test_acc.append(test_accuracy.result())\n    test_accuracy.reset_states()\n                    \nplt.plot(np.squeeze(costs))\nplt.ylabel('cost')\nplt.xlabel('iterations')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:15:25.711045Z","iopub.execute_input":"2022-09-15T15:15:25.711483Z","iopub.status.idle":"2022-09-15T15:15:51.911073Z","shell.execute_reply.started":"2022-09-15T15:15:25.711445Z","shell.execute_reply":"2022-09-15T15:15:51.909653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing machine learning models using prebuilt packages\n\nHaving run or model using TensorFlow, we will also try to train different ML models on our dataset for fraud detection. This section is mostly from [Credit Card Fraud Detection using Machine Learning & Python](http://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc) and I claim little to no originality. Again, this is a personal documentation for me to learn ML techniques.\n\nWe will first begin by loading and importing Python packages.","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa as tsa\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor\nfrom sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:18:52.397407Z","iopub.execute_input":"2022-09-15T15:18:52.398309Z","iopub.status.idle":"2022-09-15T15:18:52.847255Z","shell.execute_reply.started":"2022-09-15T15:18:52.398265Z","shell.execute_reply":"2022-09-15T15:18:52.846151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Decision Tree*","metadata":{}},{"cell_type":"code","source":"DT = DecisionTreeClassifier(max_depth=4, criterion='entropy')\nDT.fit(X_train, Y_train)\nDT_yhat = DT.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:18:54.988659Z","iopub.execute_input":"2022-09-15T15:18:54.989464Z","iopub.status.idle":"2022-09-15T15:18:59.34503Z","shell.execute_reply.started":"2022-09-15T15:18:54.989424Z","shell.execute_reply":"2022-09-15T15:18:59.343917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the accuracy of the decision tree model.","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of the Decision Tree Model is {}'.format(accuracy_score(Y_test, DT_yhat)))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:19:01.285025Z","iopub.execute_input":"2022-09-15T15:19:01.285454Z","iopub.status.idle":"2022-09-15T15:19:01.298229Z","shell.execute_reply.started":"2022-09-15T15:19:01.285415Z","shell.execute_reply":"2022-09-15T15:19:01.297044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the F1 score of the decision tree model.","metadata":{}},{"cell_type":"code","source":"print('F1 score of the Decision Tree Model is {}'.format(f1_score(Y_test, DT_yhat)))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:19:03.44101Z","iopub.execute_input":"2022-09-15T15:19:03.441981Z","iopub.status.idle":"2022-09-15T15:19:03.471902Z","shell.execute_reply.started":"2022-09-15T15:19:03.441943Z","shell.execute_reply":"2022-09-15T15:19:03.471035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the confusion matrix of the decision tree model.","metadata":{}},{"cell_type":"code","source":"confusion_matrix(Y_test, DT_yhat, labels=[0,1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:19:05.704774Z","iopub.execute_input":"2022-09-15T15:19:05.705499Z","iopub.status.idle":"2022-09-15T15:19:05.719456Z","shell.execute_reply.started":"2022-09-15T15:19:05.70546Z","shell.execute_reply":"2022-09-15T15:19:05.718527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have 55015 true positive (normal classified as normal) and 16 false positives (fraud classified as normal). Out of 28 + 74 = 102 real frauds, we missed about 16% of fraudulent transactions as fraudulent using the Decision Tree model.\n\n### *K-Nearest Neighbors*","metadata":{}},{"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors = 7)\nKNN.fit(X_train, Y_train)\nKNN_yhat = KNN.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:19:07.721128Z","iopub.execute_input":"2022-09-15T15:19:07.72152Z","iopub.status.idle":"2022-09-15T15:24:01.584889Z","shell.execute_reply.started":"2022-09-15T15:19:07.721485Z","shell.execute_reply":"2022-09-15T15:24:01.582724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the accuracy, F1 score and the confusion matrix of the K-Nearest Neighbors model.","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of the KNN model is {}'.format(accuracy_score(Y_test, KNN_yhat)))\nprint('F1 score of the KNN model is {}'.format(f1_score(Y_test, KNN_yhat)))\nconfusion_matrix(Y_test, KNN_yhat, labels = [0,1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:49:30.675325Z","iopub.execute_input":"2022-09-15T15:49:30.676013Z","iopub.status.idle":"2022-09-15T15:49:30.735529Z","shell.execute_reply.started":"2022-09-15T15:49:30.675967Z","shell.execute_reply":"2022-09-15T15:49:30.734328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Logistic Regression*","metadata":{}},{"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(X_train, Y_train)\nLR_yhat = LR.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:49:32.951889Z","iopub.execute_input":"2022-09-15T15:49:32.952927Z","iopub.status.idle":"2022-09-15T15:49:34.873169Z","shell.execute_reply.started":"2022-09-15T15:49:32.952889Z","shell.execute_reply":"2022-09-15T15:49:34.871765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy, F1 score and confusion matrix for using logistic regression is as following.","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of logistic regression is {}'.format(accuracy_score(Y_test, LR_yhat)))\nprint('F1 score of logistic regression is {}'.format(f1_score(Y_test, LR_yhat)))\nconfusion_matrix(Y_test, LR_yhat, labels = [0, 1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:49:49.199452Z","iopub.execute_input":"2022-09-15T15:49:49.200516Z","iopub.status.idle":"2022-09-15T15:49:49.246337Z","shell.execute_reply.started":"2022-09-15T15:49:49.200467Z","shell.execute_reply":"2022-09-15T15:49:49.245504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we missed 14 cases of frauds and raised false alarm for about 42 cases of normal transactions as fraudulent.\n\n### *Support Vector Machines*","metadata":{}},{"cell_type":"code","source":"svm = SVC()\nsvm.fit(X_train, Y_train)\nsvm_yhat = svm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:49:51.807839Z","iopub.execute_input":"2022-09-15T15:49:51.80881Z","iopub.status.idle":"2022-09-15T15:52:20.949327Z","shell.execute_reply.started":"2022-09-15T15:49:51.808761Z","shell.execute_reply":"2022-09-15T15:52:20.948084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy, F1 score, Confusion matrix below.","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of the SVM model is {}'.format(accuracy_score(Y_test, svm_yhat)))\nprint('F1 score of the SVM model is {}'.format(f1_score(Y_test, svm_yhat)))\nconfusion_matrix(Y_test, svm_yhat, labels=[0,1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:54:32.222282Z","iopub.execute_input":"2022-09-15T15:54:32.222797Z","iopub.status.idle":"2022-09-15T15:54:32.279146Z","shell.execute_reply.started":"2022-09-15T15:54:32.222757Z","shell.execute_reply":"2022-09-15T15:54:32.277984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Random Forest*","metadata":{}},{"cell_type":"code","source":"RF = RandomForestClassifier(max_depth = 4)\nRF.fit(X_train, Y_train)\nRF_yhat = RF.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:54:35.155211Z","iopub.execute_input":"2022-09-15T15:54:35.15567Z","iopub.status.idle":"2022-09-15T15:55:26.099182Z","shell.execute_reply.started":"2022-09-15T15:54:35.155631Z","shell.execute_reply":"2022-09-15T15:55:26.097997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy, F1 score and confusion matrix","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of the random forest model is {}'.format(accuracy_score(Y_test, RF_yhat)))\nprint('F1 score of the random forest model is {}'.format(f1_score(Y_test, RF_yhat)))\nconfusion_matrix(Y_test, RF_yhat, labels=[0,1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:55:30.646288Z","iopub.execute_input":"2022-09-15T15:55:30.646724Z","iopub.status.idle":"2022-09-15T15:55:30.69893Z","shell.execute_reply.started":"2022-09-15T15:55:30.646687Z","shell.execute_reply":"2022-09-15T15:55:30.697819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *XGBoost*","metadata":{}},{"cell_type":"code","source":"XGB = XGBClassifier(max_depth = 4)\nXGB.fit(X_train, Y_train)\nXGB_yhat = XGB.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:55:33.013966Z","iopub.execute_input":"2022-09-15T15:55:33.014398Z","iopub.status.idle":"2022-09-15T15:56:45.196532Z","shell.execute_reply.started":"2022-09-15T15:55:33.01436Z","shell.execute_reply":"2022-09-15T15:56:45.195055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy score, F1 score and Confusion Matrix","metadata":{}},{"cell_type":"code","source":"print('Accuracy score of the XGB model is {}'.format(accuracy_score(Y_test, XGB_yhat)))\nprint('F1 score of the XGB model is {}'.format(f1_score(Y_test, XGB_yhat)))\nconfusion_matrix(Y_test, XGB_yhat, labels=[0,1])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T15:57:55.703226Z","iopub.execute_input":"2022-09-15T15:57:55.703705Z","iopub.status.idle":"2022-09-15T15:57:55.754096Z","shell.execute_reply.started":"2022-09-15T15:57:55.703666Z","shell.execute_reply":"2022-09-15T15:57:55.752872Z"},"trusted":true},"execution_count":null,"outputs":[]}]}